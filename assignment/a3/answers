# Write your short answers in this file, replacing the placeholders as appropriate.
# This assignment consists of 3 parts for a total of 79 points.
# For numerical answers, copy and paste at least 5 significant figures.
# - Multiclass Text Classification (54 points)
# - Summarization Test (15 points)
# - Question Answering Test (8 points)
# - Correct submission (1 point)
# - Answer file parses (1 point)



###################################################################
###################################################################
## Multiclass Text Classification (54 points)
###################################################################
###################################################################


# ------------------------------------------------------------------
# | Section (2): Classification with a fine tuned BERT model (22 points)  | 
# ------------------------------------------------------------------

# Question 2.1 (/5): How many trainable parameters are in your dense hidden layer?
multiclass_text_classification_2_2_1: 
 - 154569

# Question 2.2 (/5): How many trainable parameters are in your classification layer?
multiclass_text_classification_2_2_2:  
 - 4040

# Question 2.3 (/5): What is the Test accuracy score you get from your model with a batch size of 8?
multiclass_text_classification_2_2_3: 
 - 0.67905

# Question 2.4 (/2): What is the key difference between the macro average F1 score and the weighted average F1 score?
# (This question is multiple choice.  Delete all but the correct answer).
multiclass_text_classification_2_2_4: 
 - The weighted average accounts for imbalance in the labels

# Question 2.5 (/5): What is the macro average F1 score you get from the classification report for batch size 8?
multiclass_text_classification_2_2_5: 
- 0.67


# ------------------------------------------------------------------
# | Section (3): Classification with some preprocessed data and the BERT model (28 points)  | 
# ------------------------------------------------------------------

# Question 3.1 (/5): What is the test accuracy you get when you run the new first stage model with only 19 classes?
multiclass_text_classification_3_3_1: 
- 0.73580

# Question 3.2 (/5): What is the F1 score you get for the combined class when you run the new first stage model with only 19 classes?
multiclass_text_classification_3_3_2: 
- 0.54

# Question 3.3 (/5): What is the macro average F1 score you get when you run the new second stage model with only 2 classes?
multiclass_text_classification_3_3_3: 
- 0.45

# Question 3.4 (/5): What is the macro average F1 score you get from the combined two-step model?
multiclass_text_classification_3_3_4: 
- 0.73

# Question 3.5 (/2): What is the difference in points between the macro weighted F1 score for the original model and the combined two-step model?
multiclass_text_classification_3_3_5: 
- 0.05

# Question 3.6 (/2): What is the new F1 score for the last category (i.e. label_to_replace, the one that had the lowest F1 score in the original model)?
multiclass_text_classification_3_3_6: 
- 0.60

# Question 3.7 (/2): What is the new F1 score for the other category that you combined with the last category in the two-step model (i.e. label_to_replace_with)?
multiclass_text_classification_3_3_7: 
- 0.77

# Question 3.8 (/2): Which metric (precision or recall) is now lower for the other category (i.e. label_to_replace_with)?
# (This question is multiple choice.  Delete all but the correct answer).
multiclass_text_classification_3_3_8: 
 - Recall is lower


# ------------------------------------------------------------------
# | Section (4): Examine some misclassified examples (4 points)  | 
# ------------------------------------------------------------------

# Question 4.1 (/2): Why do you think the two-step model got these examples wrong, when the original model got them right?
# (This question is multiple choice.  Delete all but the correct answer).
multiclass_text_classification_4_4_1: 
 - B. In the two-step process, the step 1 model overpredicted the combined class

# Question 4.2 (/2): Is there anything you might try next, to try to make the two-step model better?
# (This question is multiple choice.  Delete all but the correct answer).
multiclass_text_classification_4_4_2: 
 - C. Try both A and B



###################################################################
###################################################################
## Summarization Test (15 points)
###################################################################
###################################################################


# ------------------------------------------------------------------
# | Section (1): T5 for generic summarization (5 points)  | 
# ------------------------------------------------------------------

# Question 1.1 (/1): What num_beams value gives you the most readable output?
summarization_test_1_1_1: 
- 4

# Question 1.2 (/1): Which no_repeat_ngram_size gives the most readable output?
summarization_test_1_1_2: 
- 3

# Question 1.3 (/1): What min_length value gives you the most readable output?
summarization_test_1_1_3: 
- 50

# Question 1.4 (/1): Which max_new_tokens value gives the most readable output?
summarization_test_1_1_4: 
- 100

# Question 1.5 (/1): What is the ROUGE-L score associated with your most readable candidate?
summarization_test_1_1_5: 
- 0.33846


# ------------------------------------------------------------------
# | Section (2): Pegasus for headline summarization (5 points)  | 
# ------------------------------------------------------------------

# Question 2.1 (/1): What num_beams value gives you the most readable output?
summarization_test_2_2_1: 
- 6

# Question 2.2 (/1): Which no_repeat_ngram_size gives the most readable output?
summarization_test_2_2_2: 
- 3

# Question 2.3 (/1): What min_length value gives you the most readable output?
summarization_test_2_2_3: 
- 50

# Question 2.4 (/1): Which max_new_tokens value gives the most readable output?
summarization_test_2_2_4: 
- 100

# Question 2.5 (/1): What is the ROUGE-L score associated with your most readable candidate?
summarization_test_2_2_5: 
- 0.55882


# ------------------------------------------------------------------
# | Section (3): Pegasus for longer generation (5 points)  | 
# ------------------------------------------------------------------

# Question 3.1 (/1): What num_beams value gives you the most readable output?
summarization_test_3_3_1: 
- 6

# Question 3.2 (/1): Which no_repeat_ngram_size gives the most readable output?
summarization_test_3_3_2: 
- 3

# Question 3.3 (/1): What min_length value gives you the most readable output?
summarization_test_3_3_3: 
- 100

# Question 3.4 (/1): Which max_new_tokens value gives the most readable output?
summarization_test_3_3_4: 
- 250

# Question 3.5 (/1): What is the ROUGE-L score associated with your most readable candidate?
summarization_test_3_3_5: 
- 0.31205



###################################################################
###################################################################
## Question Answering Test (8 points)
###################################################################
###################################################################


# ------------------------------------------------------------------
# | Section (1): T5 for Question Answering (8 points)  | 
# ------------------------------------------------------------------

# Question 1.1a (/4): What is the first sentence that returns the correct answer at least 7 out of 10 times? Write your answer in the answer slot.
question_answering_test_1_1_1a: 
- The capital of Germany is <extra_id_0>.

# Question 1.1b (/4): What is the second sentence that returns the correct answer at least 7 out of 10 times? Write your answer in the answer slot.
question_answering_test_1_1_1b: 
- The primary language spoken in Germany is <extra_id_0>.
